import "dotenv/config";
import express from "express";
import cors from "cors";
import OpenAI from "openai";
import { generateImage, getWeather } from "./utils/index.js";
import tools from "./utils/tools.js";

const app = express();
app.use(cors());
app.use(express.json());

const openai = new OpenAI({
    baseURL: process.env.ZHIPUAI_BASE_URL,
    apiKey: process.env.ZHIPUAI_API_KEY,
});

// æ¨¡æ‹Ÿå¤©æ°”API

app.post("/chat", async (req, res) => {
    try {
        res.setHeader("Content-Type", "text/event-stream");
        res.setHeader("Cache-Control", "no-cache");
        res.setHeader("Connection", "keep-alive");

        let messages = [...req.body.messages];
        const stream = await openai.chat.completions.create({
            model: "glm-4-flash-250414",
            // model: "glm-4v-flash",
            messages,
            tools,
            stream: true,
        });

        let toolResponseSent = false;

        for await (const chunk of stream) {
            const toolCalls = chunk.choices[0]?.delta?.tool_calls;

            // æ£€æµ‹åˆ°å‡½æ•°è°ƒç”¨
            if (toolCalls && !toolResponseSent) {
                // æš‚åœåŽŸå§‹æµ
                stream.controller.abort();

                // èŽ·å–å‡½æ•°è°ƒç”¨å‚æ•°
                const functionCall = toolCalls[0].function;
                const args = JSON.parse(functionCall.arguments);
                // console.log("ðŸš€ ~ forawait ~ args:", args)

                let toolResponse;

                // æ ¹æ®ä¸åŒçš„å‡½æ•°è°ƒç”¨æ‰§è¡Œä¸åŒçš„æ“ä½œ
                if (functionCall.name === "get_weather") {
                    // æ‰§è¡Œå¤©æ°”æŸ¥è¯¢
                    toolResponse = await getWeather(args.location);
                } else if (functionCall.name === "generate_image") {
                    // æ‰§è¡Œå›¾ç‰‡ç”Ÿæˆ
                    toolResponse = await generateImage(args.prompt);
                }
                console.log("ðŸš€ ~ forawait ~ toolResponse:", toolResponse)

                // æž„å»ºå·¥å…·å“åº”æ¶ˆæ¯
                messages.push({
                    role: "assistant",
                    tool_calls: toolCalls,
                });

                messages.push({
                    role: "tool",
                    tool_call_id: toolCalls[0].id,
                    name: functionCall.name,
                    content: toolResponse,
                });

                // é‡æ–°å‘èµ·è¯·æ±‚èŽ·å–æœ€ç»ˆå›žå¤
                const secondStream = await openai.chat.completions.create({
                    model: "glm-4-flash-250414",
                    messages,
                    stream: true,
                });
                console.log("ðŸš€ ~ forawait ~ secondStream:", secondStream)

                // å‘é€æœ€ç»ˆå›žå¤
                for await (const finalChunk of secondStream) {
                    const content = finalChunk.choices[0]?.delta?.content || "";
                    res.write(`data: ${JSON.stringify({ content })}\n\n`);
                }

                toolResponseSent = true;
                break;
            }

            // æ­£å¸¸æ–‡æœ¬å›žå¤
            const content = chunk.choices[0]?.delta?.content || "";
            res.write(`data: ${JSON.stringify({ content })}\n\n`);
        }
    } catch (error) {
        res.status(500).json({ error: error.message });
    } finally {
        res.end();
    }
});

app.listen(8080, () => console.log("Server running on port 8080"));
